

**Notes**

Summary
1. batch normalization solves distribution changes between training batches
2. speed up training convergence

Thoughts
1. if there are big shifts between instances/batches, gradient decent direction will change often, thus slow to converge
2. 



**Reference**

<a href="http://blog.csdn.net/hjimce/article/details/50866313">深度学习（二十九）Batch Normalization 学习笔记 </a> 
